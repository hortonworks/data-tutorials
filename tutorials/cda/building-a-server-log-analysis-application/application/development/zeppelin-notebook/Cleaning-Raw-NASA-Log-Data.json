{"paragraphs":[{"text":"%md\n## Loading External Library\n\nWe will use Zeppelin's **%dep interpreter** to import external databricks: spark-csv_2.11:1.4.0 dataset.","user":"anonymous","dateUpdated":"2018-09-21T17:49:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Loading External Library</h2>\n<p>We will use Zeppelin's <strong>%dep interpreter</strong> to import external databricks: spark-csv_2.11:1.4.0 dataset.</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323491_-644150017","id":"20180910-064031_716865717","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:49:20+0000","dateFinished":"2018-09-21T17:49:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5267"},{"text":"%dep\nz.reset()\nz.load(\"com.databricks:spark-csv_2.11:1.4.0\")","user":"anonymous","dateUpdated":"2018-09-21T17:49:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@5ab5416f\n"}]},"apps":[],"jobName":"paragraph_1537549323539_1289592384","id":"20180910-064255_869358333","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:49:26+0000","dateFinished":"2018-09-21T17:49:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5268"},{"text":"%md\n## Loading the DataFrame from HDFS\n\nWe will load the dataframe from HDFS directory `/sandbox/tutorial-files/200/nifi/output/NASALogsAug1995` \nusing PySpark's `sqlContext.read.format()` function. Then we will use `.show()` function to display the \ncontent of the dataframe.","user":"anonymous","dateUpdated":"2018-09-21T17:49:47+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Loading the DataFrame from HDFS</h2>\n<p>We will load the dataframe from HDFS directory <code>/sandbox/tutorial-files/200/nifi/output/NASALogsAug1995</code>\n<br  />using PySpark's <code>sqlContext.read.format()</code> function. Then we will use <code>.show()</code> function to display the\n<br  />content of the dataframe.</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323539_1792843445","id":"20180910-064343_1029286064","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:49:47+0000","dateFinished":"2018-09-21T17:49:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5269"},{"text":"%pyspark\nfrom pyspark.sql.types import StructType, StructField, DoubleType, StringType\nschema = StructType([\n    # Represents a field in a StructType\n    StructField(\"IP\",           StringType()),\n    StructField(\"Time\",         StringType()),\n    StructField(\"Request_Type\", StringType()),\n    StructField(\"Response_Code\",StringType()),\n    StructField(\"City\",         StringType()),\n    StructField(\"Country\",      StringType()),\n    StructField(\"Isocode\",      StringType()),\n    StructField(\"Latitude\",     StringType()),\n    StructField(\"Longitude\",    StringType())\n])\n\nlogs_df = sqlContext.read \\\n                    .format(\"com.databricks.spark.csv\") \\\n                    .schema(schema) \\\n                    .option(\"header\", \"false\") \\\n                    .option(\"delimiter\", \"|\") \\\n                    .load(\"/sandbox/tutorial-files/200/nifi/output/NASALogsAug1995\")\nlogs_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-09-21T17:49:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+--------------------------+--------------------------------------------------------------------+-------------+--------------------+-------------+-------+--------+---------+\n|IP            |Time                      |Request_Type                                                        |Response_Code|City                |Country      |Isocode|Latitude|Longitude|\n+--------------+--------------------------+--------------------------------------------------------------------+-------------+--------------------+-------------+-------+--------+---------+\n|128.149.9.161 |14/Aug/1995:20:39:05 -0400|GET /shuttle/missions/missions.html HTTP/1.0                        |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:39:06 -0400|GET /images/NASA-logosmall.gif HTTP/1.0                             |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|142.23.10.104 |14/Aug/1995:20:39:09 -0400|GET /shuttle/missions/sts-69/sts-69-patch-small.gif HTTP/1.0        |200          |Kelowna             |Canada       |CA     |49.8625 |-119.5833|\n|128.149.9.161 |14/Aug/1995:20:39:24 -0400|GET /shuttle/missions/sts-26/sts-26-patch-small.gif HTTP/1.0        |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:39:24 -0400|GET /history/apollo/images/apollo-logo1.gif HTTP/1.0                |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:40:19 -0400|GET /shuttle/missions/61-b/61-b-patch-small.gif HTTP/1.0            |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:40:36 -0400|GET /shuttle/missions/61-c/61-c-patch-small.gif HTTP/1.0            |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:40:44 -0400|GET /shuttle/missions/51-l/51-l-patch-small.gif HTTP/1.0            |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:41:57 -0400|GET /history/apollo/images/apollo-small.gif HTTP/1.0                |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:42:05 -0400|GET /images/gemini-logo.gif HTTP/1.0                                |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:42:05 -0400|GET /history/apollo/images/apollo-logo.gif HTTP/1.0                 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:42:18 -0400|GET /history/gemini/gemini-viii/gemini-viii-patch-small.gif HTTP/1.0|200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:43:00 -0400|GET /history/gemini/gemini-viii/gemini-viii-info.html HTTP/1.0      |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|200.12.64.24  |14/Aug/1995:20:44:24 -0400|GET /shuttle/resources/orbiters/challenger-logo.gif HTTP/1.0        |200          |Monterrey           |Mexico       |MX     |25.6751 |-100.3185|\n|200.12.64.24  |14/Aug/1995:20:45:05 -0400|GET /images/ksclogosmall.gif HTTP/1.0                               |304          |Monterrey           |Mexico       |MX     |25.6751 |-100.3185|\n|128.149.9.161 |14/Aug/1995:20:45:41 -0400|GET /history/gemini/movies/gemini-launch.mpg HTTP/1.0               |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.149.9.161 |14/Aug/1995:20:46:15 -0400|GET /history/gemini/flight-summary.txt HTTP/1.0                     |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|\n|128.125.60.216|14/Aug/1995:20:47:06 -0400|GET /shuttle/missions/sts-69/sts-69-patch-small.gif HTTP/1.0        |200          |Los Angeles         |United States|US     |34.0266 |-118.2831|\n|128.125.60.216|14/Aug/1995:20:47:17 -0400|GET /images/launch-logo.gif HTTP/1.0                                |200          |Los Angeles         |United States|US     |34.0266 |-118.2831|\n|128.125.60.216|14/Aug/1995:20:47:38 -0400|GET /shuttle/missions/sts-69/mission-sts-69.html HTTP/1.0           |200          |Los Angeles         |United States|US     |34.0266 |-118.2831|\n+--------------+--------------------------+--------------------------------------------------------------------+-------------+--------------------+-------------+-------+--------+---------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=0"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323540_-1284785064","id":"20180910-064814_201181279","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:49:51+0000","dateFinished":"2018-09-21T17:56:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5270"},{"text":"%md\nHow many rows are display from the dataframe, `logs_df`?\nWhen we tested the demo, there was 20 rows displayed.","user":"anonymous","dateUpdated":"2018-09-21T17:57:15+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>How many rows are display from the dataframe, <code>logs_df</code>?\n<br  />When we tested the demo, there was 20 rows displayed.</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323540_-1746481","id":"20180910-065319_140545793","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:57:15+0000","dateFinished":"2018-09-21T17:57:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5271"},{"text":"%md\n## Parsing the Timestamp\nFrom the **Time** column, we will parse for the timestamp and drop the old **Time** column with the new **Timestamp** column.","user":"anonymous","dateUpdated":"2018-09-21T17:57:17+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Parsing the Timestamp</h2>\n<p>From the <strong>Time</strong> column, we will parse for the timestamp and drop the old <strong>Time</strong> column with the new <strong>Timestamp</strong> column.</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323541_364165970","id":"20180910-065630_166249826","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:57:17+0000","dateFinished":"2018-09-21T17:57:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5272"},{"text":"%pyspark\nfrom pyspark.sql.functions import udf\n\nmonths = {\n  'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12\n}\n\ndef parse_timestamp(time):\n    \"\"\" This function takes a Time string parameter of logs_df dataframe\n    Returns a string suitable for passing to CAST('timestamp') in the format YYYY-MM-DD hh:mm:ss\n    \"\"\"\n    return \"{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n      int(time[7:11]),\n      months[time[3:6]],\n      int(time[0:2]),\n      int(time[12:14]),\n      int(time[15:17]),\n      int(time[18:20])\n    )\n\nudf_parse_timestamp = udf(parse_timestamp)\n\n# Assigning the Timestamp name to the new column and dropping the old Time column\nparsed_df = logs_df.select('*',\n                udf_parse_timestamp(logs_df['Time'])\n                .cast('timestamp')\n                .alias('Timestamp')).drop('Time')  \n# Stores the dataframe in cache for the future use\nparsed_df.cache()                                  \n# Displays the results\nparsed_df.show()                                   ","user":"anonymous","dateUpdated":"2018-09-21T17:57:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+--------------------+-------------+--------------------+-------------+-------+--------+---------+-------------------+\n|            IP|        Request_Type|Response_Code|                City|      Country|Isocode|Latitude|Longitude|          Timestamp|\n+--------------+--------------------+-------------+--------------------+-------------+-------+--------+---------+-------------------+\n| 128.149.9.161|GET /shuttle/miss...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:39:05|\n| 128.149.9.161|GET /images/NASA-...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:39:06|\n| 142.23.10.104|GET /shuttle/miss...|          200|             Kelowna|       Canada|     CA| 49.8625|-119.5833|1995-08-14 20:39:09|\n| 128.149.9.161|GET /shuttle/miss...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:39:24|\n| 128.149.9.161|GET /history/apol...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:39:24|\n| 128.149.9.161|GET /shuttle/miss...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:40:19|\n| 128.149.9.161|GET /shuttle/miss...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:40:36|\n| 128.149.9.161|GET /shuttle/miss...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:40:44|\n| 128.149.9.161|GET /history/apol...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:41:57|\n| 128.149.9.161|GET /images/gemin...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:42:05|\n| 128.149.9.161|GET /history/apol...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:42:05|\n| 128.149.9.161|GET /history/gemi...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:42:18|\n| 128.149.9.161|GET /history/gemi...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:43:00|\n|  200.12.64.24|GET /shuttle/reso...|          200|           Monterrey|       Mexico|     MX| 25.6751|-100.3185|1995-08-14 20:44:24|\n|  200.12.64.24|GET /images/ksclo...|          304|           Monterrey|       Mexico|     MX| 25.6751|-100.3185|1995-08-14 20:45:05|\n| 128.149.9.161|GET /history/gemi...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:45:41|\n| 128.149.9.161|GET /history/gemi...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:46:15|\n|128.125.60.216|GET /shuttle/miss...|          200|         Los Angeles|United States|     US| 34.0266|-118.2831|1995-08-14 20:47:06|\n|128.125.60.216|GET /images/launc...|          200|         Los Angeles|United States|     US| 34.0266|-118.2831|1995-08-14 20:47:17|\n|128.125.60.216|GET /shuttle/miss...|          200|         Los Angeles|United States|     US| 34.0266|-118.2831|1995-08-14 20:47:38|\n+--------------+--------------------+-------------+--------------------+-------------+-------+--------+---------+-------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=1"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323541_-1854990901","id":"20180910-065759_1023884079","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:57:19+0000","dateFinished":"2018-09-21T17:57:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5273"},{"text":"%md\n## Cleaning the Request_Type Column\n\nCurrently the Request_type has `GET` and `HTTP/1.0` surrounding the actual data being requested, \nso we will remove these two from each line.","user":"anonymous","dateUpdated":"2018-09-21T17:57:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Cleaning the Request_Type Column</h2>\n<p>Currently the Request_type has <code>GET</code> and <code>HTTP/1.0</code> surrounding the actual data being requested,\n<br  />so we will remove these two from each line.</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323542_1286972649","id":"20180910-070806_149761259","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:57:27+0000","dateFinished":"2018-09-21T17:57:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5274"},{"text":"%pyspark\nfrom pyspark.sql.functions import split, regexp_extract\npath_df = parsed_df.select('*', regexp_extract('Request_Type', r'^.*\\w+\\s+([^\\s]+)\\s+HTTP.*', 1)\n                    .alias('Request_Path')).drop('Request_Type')\n                    \n# Cache the dataframe\npath_df.cache()\n# Displays the results\npath_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-09-21T17:57:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+-------------+--------------------+-------------+-------+--------+---------+-------------------+-------------------------------------------------------+\n|IP            |Response_Code|City                |Country      |Isocode|Latitude|Longitude|Timestamp          |Request_Path                                           |\n+--------------+-------------+--------------------+-------------+-------+--------+---------+-------------------+-------------------------------------------------------+\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:39:05|/shuttle/missions/missions.html                        |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:39:06|/images/NASA-logosmall.gif                             |\n|142.23.10.104 |200          |Kelowna             |Canada       |CA     |49.8625 |-119.5833|1995-08-14 20:39:09|/shuttle/missions/sts-69/sts-69-patch-small.gif        |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:39:24|/shuttle/missions/sts-26/sts-26-patch-small.gif        |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:39:24|/history/apollo/images/apollo-logo1.gif                |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:40:19|/shuttle/missions/61-b/61-b-patch-small.gif            |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:40:36|/shuttle/missions/61-c/61-c-patch-small.gif            |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:40:44|/shuttle/missions/51-l/51-l-patch-small.gif            |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:41:57|/history/apollo/images/apollo-small.gif                |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:42:05|/images/gemini-logo.gif                                |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:42:05|/history/apollo/images/apollo-logo.gif                 |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:42:18|/history/gemini/gemini-viii/gemini-viii-patch-small.gif|\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:43:00|/history/gemini/gemini-viii/gemini-viii-info.html      |\n|200.12.64.24  |200          |Monterrey           |Mexico       |MX     |25.6751 |-100.3185|1995-08-14 20:44:24|/shuttle/resources/orbiters/challenger-logo.gif        |\n|200.12.64.24  |304          |Monterrey           |Mexico       |MX     |25.6751 |-100.3185|1995-08-14 20:45:05|/images/ksclogosmall.gif                               |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:45:41|/history/gemini/movies/gemini-launch.mpg               |\n|128.149.9.161 |200          |La Cañada Flintridge|United States|US     |34.2313 |-118.1486|1995-08-14 20:46:15|/history/gemini/flight-summary.txt                     |\n|128.125.60.216|200          |Los Angeles         |United States|US     |34.0266 |-118.2831|1995-08-14 20:47:06|/shuttle/missions/sts-69/sts-69-patch-small.gif        |\n|128.125.60.216|200          |Los Angeles         |United States|US     |34.0266 |-118.2831|1995-08-14 20:47:17|/images/launch-logo.gif                                |\n|128.125.60.216|200          |Los Angeles         |United States|US     |34.0266 |-118.2831|1995-08-14 20:47:38|/shuttle/missions/sts-69/mission-sts-69.html           |\n+--------------+-------------+--------------------+-------------+-------+--------+---------+-------------------+-------------------------------------------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=2"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323542_1427040507","id":"20180910-071524_1178572554","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:57:29+0000","dateFinished":"2018-09-21T17:57:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5275"},{"text":"%md\n## Filtering for Most Frequent Hosts Hitting NASA Server\n\nWe want to filter on which which hosts are most frequently hitting NASA's server and then store the data into a temporary table.","user":"anonymous","dateUpdated":"2018-09-21T17:57:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Filtering for Most Frequent Hosts Hitting NASA Server</h2>\n<p>We want to filter on which which hosts are most frequently hitting NASA's server and then store the data into a temporary table.</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323542_-1408489589","id":"20180910-100601_816463436","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:57:33+0000","dateFinished":"2018-09-21T17:57:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5276"},{"text":"%pyspark\n# Group the dataframe by IP column and then counting\nmost_frequent_hosts = parsed_df.groupBy(\"IP\").count()\n\n# Displays the results\nmost_frequent_hosts.show()\n\n# Registering most_frequent_hosts variable as a temporary table\nmost_frequent_hosts.registerTempTable(\"most_frequent_hosts\")","user":"anonymous","dateUpdated":"2018-09-21T17:57:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------------+-----+\n|             IP|count|\n+---------------+-----+\n| 128.159.112.47|   74|\n| 128.200.148.26|   53|\n|  134.39.70.204|   94|\n|  163.205.80.44|  166|\n| 205.197.248.13|   21|\n|  147.147.79.82|    6|\n|140.174.227.129|    7|\n|   138.86.14.23|   29|\n| 159.121.38.213|   26|\n|   152.43.32.90|   15|\n| 158.111.78.158|   65|\n| 141.211.22.120|    9|\n|   192.86.22.98|    4|\n|  204.148.32.16|   10|\n|  164.116.78.80|   29|\n|   168.169.1.50|    4|\n| 147.222.27.131|   16|\n| 132.170.244.49|   12|\n|   198.16.21.89|   10|\n| 159.153.128.27|   22|\n+---------------+-----+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=3"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323543_-1607606052","id":"20180910-100934_1747121664","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:57:36+0000","dateFinished":"2018-09-21T17:58:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5277"},{"text":"%md\n## Filtering for Count of Each Response Code\n\nOur aim is to find the amoutn of times that each response code has occurred and store the result into a temporary table for later use.","user":"anonymous","dateUpdated":"2018-09-21T17:58:50+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Filtering for Count of Each Response Code</h2>\n<p>Our aim is to find the amoutn of times that each response code has occurred and store the result into a temporary table for later use.</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323546_-1571881874","id":"20180910-101241_2083450251","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:58:50+0000","dateFinished":"2018-09-21T17:58:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5278"},{"text":"%pyspark\n# Groups the dataframe by Response_Code column and then counting\nstatus_count = path_df.groupBy('Response_Code').count()\n# Displays the results\nstatus_count.show()\n# Registering status_count variable as a temporary table\nstatus_count.registerTempTable(\"status_count\")","user":"anonymous","dateUpdated":"2018-09-21T17:58:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+-----+\n|Response_Code|count|\n+-------------+-----+\n|          200|83810|\n|         null|   84|\n|          302| 1447|\n|          404|  493|\n|          403|    2|\n|          304| 6378|\n+-------------+-----+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=4","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=5","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=6","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=7","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=8"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323547_874339832","id":"20180910-101427_1183814886","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:58:52+0000","dateFinished":"2018-09-21T17:59:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5279"},{"text":"%md\n## Filtering Records Where Response Code is 200","user":"anonymous","dateUpdated":"2018-09-21T17:59:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Filtering Records Where Response Code is 200</h2>\n"}]},"apps":[],"jobName":"paragraph_1537549323547_770697502","id":"20180910-071834_1541655729","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:59:03+0000","dateFinished":"2018-09-21T17:59:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5280"},{"text":"%pyspark\n# Creating dataframe where Response Code is 200\nsuccess_logs_df = parsed_df.select('*').filter(path_df['Response_Code'] == 200)\n# Cache the dataframe\nsuccess_logs_df.cache()\n# Displays the results\nsuccess_logs_df.show()","user":"anonymous","dateUpdated":"2018-09-21T17:59:06+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+--------------------+-------------+--------------------+-------------+-------+--------+---------+-------------------+\n|            IP|        Request_Type|Response_Code|                City|      Country|Isocode|Latitude|Longitude|          Timestamp|\n+--------------+--------------------+-------------+--------------------+-------------+-------+--------+---------+-------------------+\n| 128.149.9.161|GET /shuttle/miss...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:39:05|\n| 128.149.9.161|GET /images/NASA-...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:39:06|\n| 142.23.10.104|GET /shuttle/miss...|          200|             Kelowna|       Canada|     CA| 49.8625|-119.5833|1995-08-14 20:39:09|\n| 128.149.9.161|GET /shuttle/miss...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:39:24|\n| 128.149.9.161|GET /history/apol...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:39:24|\n| 128.149.9.161|GET /shuttle/miss...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:40:19|\n| 128.149.9.161|GET /shuttle/miss...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:40:36|\n| 128.149.9.161|GET /shuttle/miss...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:40:44|\n| 128.149.9.161|GET /history/apol...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:41:57|\n| 128.149.9.161|GET /images/gemin...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:42:05|\n| 128.149.9.161|GET /history/apol...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:42:05|\n| 128.149.9.161|GET /history/gemi...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:42:18|\n| 128.149.9.161|GET /history/gemi...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:43:00|\n|  200.12.64.24|GET /shuttle/reso...|          200|           Monterrey|       Mexico|     MX| 25.6751|-100.3185|1995-08-14 20:44:24|\n| 128.149.9.161|GET /history/gemi...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:45:41|\n| 128.149.9.161|GET /history/gemi...|          200|La Cañada Flintridge|United States|     US| 34.2313|-118.1486|1995-08-14 20:46:15|\n|128.125.60.216|GET /shuttle/miss...|          200|         Los Angeles|United States|     US| 34.0266|-118.2831|1995-08-14 20:47:06|\n|128.125.60.216|GET /images/launc...|          200|         Los Angeles|United States|     US| 34.0266|-118.2831|1995-08-14 20:47:17|\n|128.125.60.216|GET /shuttle/miss...|          200|         Los Angeles|United States|     US| 34.0266|-118.2831|1995-08-14 20:47:38|\n|165.227.10.104|GET /shuttle/miss...|          200|            New York|United States|     US| 40.7214| -74.0052|1995-08-14 20:49:36|\n+--------------+--------------------+-------------+--------------------+-------------+-------+--------+---------+-------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=9"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323547_-1967415724","id":"20180910-080349_1046169025","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:59:06+0000","dateFinished":"2018-09-21T17:59:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5281"},{"text":"%pyspark\nfrom pyspark.sql.functions import hour\n# Extracting the Hour\nsuccess_logs_by_hours_df = success_logs_df.select(hour('Timestamp').alias('Hour')).groupBy('Hour').count()\n# Displays the results\nsuccess_logs_by_hours_df.show()\n# Registering Temporary Table that can be used by SQL interpreter\nsuccess_logs_by_hours_df.registerTempTable(\"success_logs_by_hours_df\")","user":"anonymous","dateUpdated":"2018-09-21T17:59:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+-----+\n|Hour|count|\n+----+-----+\n|  12| 6677|\n|  22| 2350|\n|   1| 1521|\n|  13| 6148|\n|  16| 5556|\n|   6| 1908|\n|   3| 1261|\n|  20| 2577|\n|   5| 1646|\n|  19| 2873|\n|  15| 6525|\n|  17| 3987|\n|   9| 4073|\n|   4| 1108|\n|   8| 3350|\n|  23| 1957|\n|   7| 2906|\n|  10| 5269|\n|  21| 2460|\n|  11| 5875|\n+----+-----+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=10","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=11","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=12","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=13","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=14","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=15"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323548_81199979","id":"20180910-101734_1697032482","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:59:10+0000","dateFinished":"2018-09-21T17:59:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5282"},{"text":"%md\n## Cleaning the Request_Path Column for Type Extensions\nRequest_Path column contains the type extension","user":"anonymous","dateUpdated":"2018-09-21T17:59:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Cleaning the Request_Path Column for Type Extensions</h2>\n<p>Request_Path column contains the type extension</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323548_1384173580","id":"20180910-080527_311900773","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:59:21+0000","dateFinished":"2018-09-21T17:59:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5283"},{"text":"%pyspark\n# Show the current Request_Path Column before applying cleansing\nfrom pyspark.sql.functions import split, regexp_extract\nextension_df = path_df.select(regexp_extract('Request_Path', '(\\\\.[^.]+)$',1).alias('Extension'))\nextension_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-09-21T17:59:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+\n|Extension|\n+---------+\n|.html    |\n|.gif     |\n|.gif     |\n|.gif     |\n|.gif     |\n|.gif     |\n|.gif     |\n|.gif     |\n|.gif     |\n|.gif     |\n|.gif     |\n|.gif     |\n|.html    |\n|.gif     |\n|.gif     |\n|.mpg     |\n|.txt     |\n|.gif     |\n|.gif     |\n|.html    |\n+---------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=16"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323548_1349469021","id":"20180910-082813_620573294","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:59:24+0000","dateFinished":"2018-09-21T17:59:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5284"},{"text":"%md\n## How Should We Clean this Request_Path Column?\n\nIn the column, we see that each extension has a dot (.) and there may be some rows in the column that are blank. \nWe will replace this character with a blank character.","user":"anonymous","dateUpdated":"2018-09-21T17:59:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>How Should We Clean this Request_Path Column?</h2>\n<p>In the column, we see that each extension has a dot (.) and there may be some rows in the column that are blank.\n<br  />We will replace this character with a blank character.</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323549_-1056249961","id":"20180910-083007_44889580","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:59:32+0000","dateFinished":"2018-09-21T17:59:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5285"},{"text":"%pyspark\nfrom pyspark.sql.functions import split, regexp_replace\n# Replace the dot character with the blank character\nextension_df = extension_df.select(regexp_replace('Extension', '\\.','').alias('Extension'))\n# Displays the results\nextension_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-09-21T17:59:37+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+\n|Extension|\n+---------+\n|html     |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|html     |\n|gif      |\n|gif      |\n|mpg      |\n|txt      |\n|gif      |\n|gif      |\n|html     |\n+---------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=17"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323549_1823132501","id":"20180910-083205_1947020","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:59:37+0000","dateFinished":"2018-09-21T17:59:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5286"},{"text":"%pyspark\nfrom pyspark.sql.functions import *\n# Replaces the blank value with the value 'None' in Extension\nextension_df = extension_df.replace('', 'None', 'Extension').alias('Extension')\nextension_df.cache()\n# Shows the results\nextension_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-09-21T17:59:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+\n|Extension|\n+---------+\n|html     |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|gif      |\n|html     |\n|gif      |\n|gif      |\n|mpg      |\n|txt      |\n|gif      |\n|gif      |\n|html     |\n+---------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=18"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323549_-878881700","id":"20180910-090614_821415297","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:59:39+0000","dateFinished":"2018-09-21T17:59:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5287"},{"text":"%pyspark\n# Groups the dataframe by Extension and then count the rows\nextension_df_count = extension_df.groupBy('Extension').count()\n# Displays the results\nextension_df_count.show()\n# Registers the temporary table\nextension_df_count.registerTempTable('extension_df_count')","user":"anonymous","dateUpdated":"2018-09-21T17:59:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----+\n|           Extension|count|\n+--------------------+-----+\n|                 art|    1|\n|   pl?MIR-Rendezvous|    2|\n|                 jpg| 1611|\n|                 map|    8|\n|                 wav|   76|\n|       pl?challenger|    2|\n|pl?august+1995+la...|    1|\n|           pl?STS-73|    2|\n|           pl?TDRS-A|    1|\n|        pl?souvenirs|    1|\n|             pl?IMAX|   16|\n|           pl?sts-69|    1|\n|                orig|    1|\n|  pl?launch+and+pass|    1|\n|           pl?STS-76|    1|\n|            news/138|    1|\n|   pl?shuttle+status|    1|\n|   pl?see+AND+launch|    1|\n|             pl?ccms|    1|\n|             pl?IPMP|    1|\n+--------------------+-----+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=19","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=20","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=21"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1537549323550_-2001363291","id":"20180910-090859_1654396960","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:59:44+0000","dateFinished":"2018-09-21T17:59:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5288"},{"text":"%md\n## Convert path_df from DataFrame to Hive Table for Map Visualization Prep\n\nCreate a temporary table for DataFrame `path_df`, then create a `Hive Table` based on that DataFrame, \nwhich can be used for data visualization with external tools, such as Tableau, Microsoft Excel, etc.","user":"anonymous","dateUpdated":"2018-09-21T17:59:52+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Convert path_df from DataFrame to Hive Table for Map Visualization Prep</h2>\n<p>Create a temporary table for DataFrame <code>path_df</code>, then create a <code>Hive Table</code> based on that DataFrame,\n<br  />which can be used for data visualization with external tools, such as Tableau, Microsoft Excel, etc.</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323550_307491818","id":"20180910-092608_1221292337","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T17:59:52+0000","dateFinished":"2018-09-21T17:59:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5289"},{"text":"%pyspark\npath_df.registerTempTable(\"path_df\")","user":"anonymous","dateUpdated":"2018-09-21T18:22:01+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1537549323550_1457848922","id":"20180910-102748_357988619","dateCreated":"2018-09-21T17:02:03+0000","dateStarted":"2018-09-21T18:22:01+0000","dateFinished":"2018-09-21T18:22:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5290"},{"text":"%md\n## Create Visualizations Based on Our Data Preprocessing\n\nIn the next phase of development, we can visualize the data we cleaned and refined from the tables they were stored into.","user":"anonymous","dateUpdated":"2018-09-21T17:02:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Create Visualizations Based on Our Data Preprocessing</h2>\n<p>In the next phase of development, we can visualize the data we cleaned and refined from the tables they were stored into.</p>\n"}]},"apps":[],"jobName":"paragraph_1537549323551_-1177805270","id":"20180910-103009_1243123415","dateCreated":"2018-09-21T17:02:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5291"}],"name":"Cleaning-Raw-NASA-Log-Data","id":"2DQK9UJ3F","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}