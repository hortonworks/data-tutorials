{"paragraphs":[{"text":"%md\n## Loading External Library\n\nWe will use Zeppelin's **%dep interpreter** to import external databricks: spark-csv_2.11:1.4.0 dataset.","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Loading External Library</h2>\n<p>We will use Zeppelin's <strong>%dep interpreter</strong> to import external databricks: spark-csv_2.11:1.4.0 dataset.</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519868_-385382243","id":"20180910-064031_716865717","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7289"},{"text":"%dep\nz.reset()\nz.load(\"com.databricks:spark-csv_2.11:1.4.0\")","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519869_-414321201","id":"20180910-064255_869358333","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7290"},{"text":"%md\n## Loading the DataFrame from HDFS\n\nWe will load the dataframe from HDFS directory `/sandbox/tutorial-files/200/nifi/output/NASALogsAug1995` \nusing PySpark's `sqlContext.read.format()` function. Then we will use `.show()` function to display the \ncontent of the dataframe.","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Loading the DataFrame from HDFS</h2>\n<p>We will load the dataframe from HDFS directory <code>/sandbox/tutorial-files/200/nifi/output/NASALogsAug1995</code>\n<br  />using PySpark's <code>sqlContext.read.format()</code> function. Then we will use <code>.show()</code> function to display the\n<br  />content of the dataframe.</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519869_-1795253572","id":"20180910-064343_1029286064","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7291"},{"text":"%pyspark\nfrom pyspark.sql.types import StructType, StructField, DoubleType, StringType\nschema = StructType([\n    # Represents a field in a StructType\n    StructField(\"IP\",           StringType()),\n    StructField(\"Time\",         StringType()),\n    StructField(\"Request_Type\", StringType()),\n    StructField(\"Response_Code\",StringType()),\n    StructField(\"City\",         StringType()),\n    StructField(\"Country\",      StringType()),\n    StructField(\"Isocode\",      StringType()),\n    StructField(\"Latitude\",     StringType()),\n    StructField(\"Longitude\",    StringType())\n])\n\nlogs_df = sqlContext.read \\\n                    .format(\"com.databricks.spark.csv\") \\\n                    .schema(schema) \\\n                    .option(\"header\", \"false\") \\\n                    .option(\"delimiter\", \"|\") \\\n                    .load(\"/sandbox/tutorial-files/200/nifi/output/NASALogsAug1995\")\nlogs_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519869_198260796","id":"20180910-064814_201181279","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7292"},{"text":"%md\nHow many rows are display from the dataframe, `logs_df`?\nWhen we tested the demo, there was 20 rows displayed.","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>How many rows are display from the dataframe, <code>logs_df</code>?\n<br  />When we tested the demo, there was 20 rows displayed.</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519870_1996289202","id":"20180910-065319_140545793","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7293"},{"text":"%md\n## Parsing the Timestamp\nFrom the **Time** column, we will parse for the timestamp and drop the old **Time** column with the new **Timestamp** column.","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Parsing the Timestamp</h2>\n<p>From the <strong>Time</strong> column, we will parse for the timestamp and drop the old <strong>Time</strong> column with the new <strong>Timestamp</strong> column.</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519870_-1402838793","id":"20180910-065630_166249826","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7294"},{"text":"%pyspark\nfrom pyspark.sql.functions import udf\n\nmonths = {\n  'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12\n}\n\ndef parse_timestamp(time):\n    \"\"\" This function takes a Time string parameter of logs_df dataframe\n    Returns a string suitable for passing to CAST('timestamp') in the format YYYY-MM-DD hh:mm:ss\n    \"\"\"\n    return \"{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n      int(time[7:11]),\n      months[time[3:6]],\n      int(time[0:2]),\n      int(time[12:14]),\n      int(time[15:17]),\n      int(time[18:20])\n    )\n\nudf_parse_timestamp = udf(parse_timestamp)\n\n# Assigning the Timestamp name to the new column and dropping the old Time column\nparsed_df = logs_df.select('*',\n                udf_parse_timestamp(logs_df['Time'])\n                .cast('timestamp')\n                .alias('Timestamp')).drop('Time')  \n# Stores the dataframe in cache for the future use\nparsed_df.cache()                                  \n# Displays the results\nparsed_df.show()                                   ","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519870_-401688419","id":"20180910-065759_1023884079","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7295"},{"text":"%md\n## Cleaning the Request_Type Column\n\nCurrently the Request_type has `GET` and `HTTP/1.0` surrounding the actual data being requested, \nso we will remove these two from each line.","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Cleaning the Request_Type Column</h2>\n<p>Currently the Request_type has <code>GET</code> and <code>HTTP/1.0</code> surrounding the actual data being requested,\n<br  />so we will remove these two from each line.</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519870_1098767744","id":"20180910-070806_149761259","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7296"},{"text":"%pyspark\nfrom pyspark.sql.functions import split, regexp_extract\npath_df = parsed_df.select('*', regexp_extract('Request_Type', r'^.*\\w+\\s+([^\\s]+)\\s+HTTP.*', 1)\n                    .alias('Request_Path')).drop('Request_Type')\n                    \n# Cache the dataframe\npath_df.cache()\n# Displays the results\npath_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519870_880251360","id":"20180910-071524_1178572554","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7297"},{"text":"%md\n## Filtering for Most Frequent Hosts Hitting NASA Server\n\nWe want to filter on which which hosts are most frequently hitting NASA's server and then store the data into a temporary table.","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Filtering for Most Frequent Hosts Hitting NASA Server</h2>\n<p>We want to filter on which which hosts are most frequently hitting NASA's server and then store the data into a temporary table.</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519870_413332274","id":"20180910-100601_816463436","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7298"},{"text":"%pyspark\n# Group the dataframe by IP column and then counting\nmost_frequent_hosts = parsed_df.groupBy(\"IP\").count()\n\n# Displays the results\nmost_frequent_hosts.show()\n\n# Registering most_frequent_hosts variable as a temporary table\nmost_frequent_hosts.registerTempTable(\"most_frequent_hosts\")","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519871_-1911449165","id":"20180910-100934_1747121664","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7299"},{"text":"%md\n## Filtering for Count of Each Response Code\n\nOur aim is to find the amoutn of times that each response code has occurred and store the result into a temporary table for later use.","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Filtering for Count of Each Response Code</h2>\n<p>Our aim is to find the amoutn of times that each response code has occurred and store the result into a temporary table for later use.</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519871_-1484348240","id":"20180910-101241_2083450251","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7300"},{"text":"%pyspark\n# Groups the dataframe by Response_Code column and then counting\nstatus_count = path_df.groupBy('Response_Code').count()\n# Displays the results\nstatus_count.show()\n# Registering status_count variable as a temporary table\nstatus_count.registerTempTable(\"status_count\")","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519871_1945179243","id":"20180910-101427_1183814886","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7301"},{"text":"%md\n## Filtering Records Where Response Code is 200","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Filtering Records Where Response Code is 200</h2>\n"}]},"apps":[],"jobName":"paragraph_1538130519871_2112442567","id":"20180910-071834_1541655729","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7302"},{"text":"%pyspark\n# Creating dataframe where Response Code is 200\nsuccess_logs_df = parsed_df.select('*').filter(path_df['Response_Code'] == 200)\n# Cache the dataframe\nsuccess_logs_df.cache()\n# Displays the results\nsuccess_logs_df.show()","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519872_-1370658619","id":"20180910-080349_1046169025","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7303"},{"text":"%pyspark\nfrom pyspark.sql.functions import hour\n# Extracting the Hour\nsuccess_logs_by_hours_df = success_logs_df.select(hour('Timestamp').alias('Hour')).groupBy('Hour').count()\n# Displays the results\nsuccess_logs_by_hours_df.show()\n# Registering Temporary Table that can be used by SQL interpreter\nsuccess_logs_by_hours_df.registerTempTable(\"success_logs_by_hours_df\")","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519872_1483806706","id":"20180910-101734_1697032482","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7304"},{"text":"%md\n## Cleaning the Request_Path Column for Type Extensions\nRequest_Path column contains the type extension","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Cleaning the Request_Path Column for Type Extensions</h2>\n<p>Request_Path column contains the type extension</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519872_130880481","id":"20180910-080527_311900773","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7305"},{"text":"%pyspark\n# Show the current Request_Path Column before applying cleansing\nfrom pyspark.sql.functions import split, regexp_extract\nextension_df = path_df.select(regexp_extract('Request_Path', '(\\\\.[^.]+)$',1).alias('Extension'))\nextension_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519872_633199932","id":"20180910-082813_620573294","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7306"},{"text":"%md\n## How Should We Clean this Request_Path Column?\n\nIn the column, we see that each extension has a dot (.) and there may be some rows in the column that are blank. \nWe will replace this character with a blank character.","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>How Should We Clean this Request_Path Column?</h2>\n<p>In the column, we see that each extension has a dot (.) and there may be some rows in the column that are blank.\n<br  />We will replace this character with a blank character.</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519872_-1814276321","id":"20180910-083007_44889580","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7307"},{"text":"%pyspark\nfrom pyspark.sql.functions import split, regexp_replace\n# Replace the dot character with the blank character\nextension_df = extension_df.select(regexp_replace('Extension', '\\.','').alias('Extension'))\n# Displays the results\nextension_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519873_392207073","id":"20180910-083205_1947020","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7308"},{"text":"%pyspark\nfrom pyspark.sql.functions import *\n# Replaces the blank value with the value 'None' in Extension\nextension_df = extension_df.replace('', 'None', 'Extension').alias('Extension')\nextension_df.cache()\n# Shows the results\nextension_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519873_-935282618","id":"20180910-090614_821415297","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7309"},{"text":"%pyspark\n# Groups the dataframe by Extension and then count the rows\nextension_df_count = extension_df.groupBy('Extension').count()\n# Displays the results\nextension_df_count.show()\n# Registers the temporary table\nextension_df_count.registerTempTable('extension_df_count')","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1538130519873_-1684707596","id":"20180910-090859_1654396960","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7310"},{"text":"%md\n## Convert path_df from DataFrame to Hive Table for Map Visualization Prep\n\nCreate a temporary table for DataFrame `path_df`, then create a `Hive Table` based on that DataFrame, \nwhich can be used for data visualization with external tools, such as Tableau, Microsoft Excel, etc.","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Convert path_df from DataFrame to Hive Table for Map Visualization Prep</h2>\n<p>Create a temporary table for DataFrame <code>path_df</code>, then create a <code>Hive Table</code> based on that DataFrame,\n<br  />which can be used for data visualization with external tools, such as Tableau, Microsoft Excel, etc.</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519873_607645034","id":"20180910-092608_1221292337","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7311"},{"text":"%pyspark\npath_df.registerTempTable(\"path_df\")","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1538130519873_-1238702093","id":"20180910-102748_357988619","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7312"},{"text":"%md\n## Create Visualizations Based on Our Data Preprocessing\n\nIn the next phase of development, we can visualize the data we cleaned and refined from the tables they were stored into.","user":"anonymous","dateUpdated":"2018-09-28T10:28:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Create Visualizations Based on Our Data Preprocessing</h2>\n<p>In the next phase of development, we can visualize the data we cleaned and refined from the tables they were stored into.</p>\n"}]},"apps":[],"jobName":"paragraph_1538130519874_-1167068237","id":"20180910-103009_1243123415","dateCreated":"2018-09-28T10:28:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7313"}],"name":"Cleaning-Raw-NASA-Log-Data","id":"2DTHQVXFH","noteParams":{},"noteForms":{},"angularObjects":{"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}